<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Home  | Classifying Household Income from US Census Data</title>
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">

    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.40.3" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="/dist/css/app.ab4b67a3ea25990fa8279f3b7ef08b61.css" rel="stylesheet">
    

    

    
      
    

    

    <meta property="og:title" content="Classifying Household Income from US Census Data" />
<meta property="og:description" content="Some fun with boosted classification algorithms" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://katherinechandler.io/post/kaggle-census-boosted-classifiers/" />



<meta property="article:published_time" content="2018-07-10T10:58:08-04:00"/>

<meta property="article:modified_time" content="2018-07-10T10:58:08-04:00"/>











<meta itemprop="name" content="Classifying Household Income from US Census Data">
<meta itemprop="description" content="Some fun with boosted classification algorithms">


<meta itemprop="datePublished" content="2018-07-10T10:58:08-04:00" />
<meta itemprop="dateModified" content="2018-07-10T10:58:08-04:00" />
<meta itemprop="wordCount" content="1544">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Classifying Household Income from US Census Data"/>
<meta name="twitter:description" content="Some fun with boosted classification algorithms"/>

  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  
  
  <header class="cover bg-top" style="background-image: url('http://katherinechandler.io/images/census.jpg');">
    <div class="pb3-m pb6-l bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="http://katherinechandler.io" class="f3 fw2 hover-white no-underline white-90 dib">
      Home
    </a>
    <div class="flex-l items-center">
      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/about/" title="About Katherine page">
              About Katherine
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/post/" title="Data Blog page">
              Data Blog
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/travel_blog/" title="Travel Blog page">
              Travel Blog
            </a>
          </li>
          
        </ul>
      
      







  <a href="https://github.com/katherinechandler" class="link-transition github link dib z-999 pt3 pt0-l mr2" title="Github link">
    <svg  height="32px"  style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
  <path d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z"/>
</svg>

  </a>


    </div>
  </div>
</nav>

      <div class="tc-l pv6 ph3 ph4-ns">
        <h1 class="f2 f1-l fw2 white-90 mb0 lh-title">Classifying Household Income from US Census Data</h1>
        
          <h2 class="fw1 f5 f3-l white-80 measure-wide-l center lh-copy mt3 mb4">
            Some fun with boosted classification algorithms
          </h2>
        
      </div>
    </div>
  </header>



    <main class="pb7" role="main">
      
  <article class="flex-l flex-wrap justify-between mw8 center">

    <header class="mt4 w-100">
      <p class="f6 b helvetica tracked">
          
        DATA BLOG
      </p>
      <h1 class="f1 athelas mb1">Classifying Household Income from US Census Data</h1>
      
      <time class="f6 mv4 dib tracked" datetime="2018-07-10T10:58:08-04:00">July 10, 2018</time>
    </header>

    <main class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l">

<p>With this project I dug into classification algorithms and selected a data set from the <a href="https://www.kaggle.com/uciml/adult-census-income/home">&lsquo;Adult Census Income&rsquo;</a> Kaggle Challenge. The data supplied for this challenge data was extracted from the 1994 census and contains aggregated demographic information (associated with a class &lsquo;weight&rsquo;) for US households. The prediction task is a binary classification to determine if a given household&rsquo;s annual income is above $50,000 a year.</p>

<p>In this blog I present a high-level overview of my analysis. For in-depth feature selection and hyper-parameter tuning code, see my <a href="https://github.com/katherinechandler/Census_Kaggle/tree/master">Census Income Classification</a> notebook in this repo.</p>

<p>My approach for this analysis was as follows:</p>

<ol>
<li>Initial exploratory analysis</li>
<li>Initial assessment of ML classification estimators

<ul>
<li>Quality was assessed by calculating precision/recall scores and comparing ROC curves</li>
</ul></li>
<li>Recursive feature selection (RFE) using estimator shortlist to rank feature importance</li>
<li>Re-assess short-listed models with new feature set</li>
<li>Perform parameter tuning using GridSearchCV on best models</li>
<li>Identify best model</li>
</ol>

<p>The best model identified in this analysis was the GradientBoostingClassifier (accuracy = 87.4%), with LightGBMClassifier coming in a close second (accuracy = 87.3%). Both of these algorithms are &lsquo;boosted&rsquo; algorithms, which means they use multiple weakly-correlated features to build predictions. The success of this type of model indicates that while some demographic features may be strong indicators of household income, the complete story of household income in the US is made up of many weakly correlated but significant factors.</p>

<h3 id="initial-exploratory-analysis">Initial exploratory analysis</h3>

<p>The census data provided with this Kaggle Challenge was nice and clean, so after minimal data wrangling I did some exploration. This data set contains 32,561 observations, each of which represents a weighted demographic group described by 14 variables (defined <a href="https://www.census.gov/prod/2003pubs/censr-5.pdf">here</a>). Of the 32,561 demographic groups described in this data set, only 7,841 groups (24.1%) are in the &lsquo;greater than $50K annually&rsquo; category.</p>

<figure>
  <img src="/images/census_images/census_fig1.png" alt="Fig1" style='width:75%'>
  <figcaption>Figure 1. The ratio of demographic groups in each income category</figcaption>
</figure>

<p>The income class data tells us that if we were to always predict &lsquo;less than $50K annually&rsquo;, the model would be correct 75.9% of the time. This imbalance makes it easy to overestimate the quality of a model when predicting the minority class, so the accuracy score of potential classification models needs to be considered in the context of this 75.9%-baseline.</p>

<p>I plotted a Pearson&rsquo;s correlation matrix to determine possible relationships between the variables and the prediction target (income).</p>

<figure>
  <img src="/images/census_images/census_fig3.png" alt="Fig1" style='width:75%'>
  <figcaption>Figure 2. Pearson's correlation heatmap</figcaption>
</figure>

<p>Examining this heat-map shows some small to medium correlations in the data. Age-income are weakly correlated (&rho; = 0.234), as are capital_gain-income (&rho; = 0.223) and capital_lost-income (&rho; = 0.151). These correlations make intuitive sense, as people who are older often have higher earning power, and households with higher disposable income are more likely to invest money and experience capital gains and losses. Similarly, there is an expected correlation between income-hours_per_week (&rho; = 0.230), indicating that people who work more hours often make more money than those who work fewer hours. In addition to these small correlations, the Pearson&rsquo;s correlation test found a medium-strength correlation between education and income (&rho; = 0.335), again fitting the intuitive assumptions about income potential.</p>

<p>Demographic features like race, country of origin, or marital/familial status were not addressed in the correlation matrix since they are categorical variables, but I suspected that the relationship between categorical demographic factors and income might be subtle. This idea that multiple weak factors might provide predictive power lead me to include several &lsquo;boosted&rsquo; algorithms in my initial assessment of Machine Learning (ML) estimators.</p>

<h3 id="initial-assessment-of-ml-classification-estimators">Initial assessment of ML classification estimators</h3>

<p>I generated a list of 8 classification algorithms for a preliminary screen. All of these algorithms are in the sklearn package except the lgb.LGBMClassifier, which is a boosted tree model documented <a href="https://lightgbm.readthedocs.io/en/latest/index.html#">here</a>.
<br />
<br />
<code>
estimator_list = [LogisticRegression, RandomForestClassifier, SGDClassifier, 
                  lgb.LGBMClassifier, ExtraTreesClassifier, 
                  GradientBoostingClassifier, SVC, LinearSVC]
</code>
<br />
<br />
I assessed the algorithms using <code>sklearn.metrics</code> (<code>classification_report</code>, <code>precision_score</code>, and <code>recall_score</code>) in my training-cross validation functions and plotted the Receiver Operating Characteristics (ROC) curve of each estimator.</p>

<figure>
  <img src="/images/census_images/census_fig5.png" alt="Fig1" style='width:200%'>
  <figcaption>Figure 3. The ROC curve of selected estimators in predicting income class</figcaption>
</figure>

<p>The initial exploration of models shows that 1) precision is highest for LGBMClassifier (0.787) and SVC (0.828), 2) recall is highest for LGBMClassifier (0.645) and SGDClassifier (0.831), and 3) the ROC curve of the LGBMClassifier and GradientBoostingClassifier look nearly equivalently good.</p>

<p>As I suspected, boosted models work well for this classification task.</p>

<h3 id="recursive-feature-selection-rfe-using-estimator-shortlist-to-rank-feature-importance">Recursive feature selection (RFE) using estimator shortlist to rank feature importance</h3>

<p>I narrowed down to a &lsquo;short-list&rsquo; of 4 classifiers to perform feature selection.
<br />
<br />
<code>estimator_shortlist = [LGBMClassifier, 
                      SVC, 
                      SGDClassifier, 
                      GradientBoostingClassifier]</code><br />
<br />
I performed RFE for each estimator and saved the ranking scores to a pandas data frame.</p>

<pre><code>def rank_features(X, y, estimator, feature_list):
    est = estimator()
    rfe = RFE(est)
    rfe = rfe.fit(X,y)
    keys = feature_list
    values = rfe.ranking_
    dictionary = dict(zip(keys, values))
    scores = pd.DataFrame.from_dict(dictionary, orient='index')
    scores.columns = ['{}_rank'.format(estimator.__name__)]
    return(scores)
</code></pre>

<p>I evaluated feature rankings for each estimator. Features consistently ranked as important are likely going to be relevant for building a final model and provide insight into the nature of the data. Features that are consistently ranked as important are <code>'workclass', 'marital_status', 'occupation', 'capital_gain', 'capital_loss', 'education_num', 'hours_per_week', 'gender'</code>, and <code>'race'</code>. <code>'Age'</code> is ranked as important by the LGBMClassifier and GradientBoostingClassifier models. Specific <code>'native_country'</code> features are ranked as important to individual models, but overall these features are less important.</p>

<p>Based on this assessment, I removed &lsquo;native_country&rsquo; from the feature list and re-assessed my estimator shortlist using precision/recall assessment and ROC curve comparisons as described above.</p>

<h3 id="re-assess-short-listed-models-with-new-feature-set">Re-assess short-listed models with new feature set</h3>

<p>There is only a slight change in performance for the estimators after feature refinement. A few of the estimators get slightly better but some actually get poorer.</p>

<p><code>Precision</code> : <br />
<code>'GradientBoostingClassifier': 0.7990654205607477 (before)</code><br />
<code>'GradientBoostingClassifier': 0.79858657243816256 (after)</code><br />
<code>'LGBMClassifier': 0.78727841501564133 (before)</code><br />
<code>'LGBMClassifier': 0.78296988577362414 (after)</code><br />
<code>'SGDClassifier': 0.51455026455026454 (before)</code><br />
<code>'SGDClassifier': 0.41291263906620462 (after)</code><br />
<code>'SVC': 0.82787958115183247 (before)</code><br />
<code>'SVC': 0.8098591549295775 (after)</code></p>

<p><code>Recall</code>:<br />
<code>'GradientBoostingClassifier': 0.58461538461538465 (before)</code><br />
<code>'GradientBoostingClassifier': 0.57948717948717954 (after)</code><br />
<code>'LGBMClassifier': 0.64529914529914534 (before)</code><br />
<code>'LGBMClassifier': 0.64444444444444449 (after)</code><br />
<code>'SGDClassifier': 0.83119658119658124 (before)</code><br />
<code>'SGDClassifier': 0.96752136752136753 (after)</code><br />
<code>'SVC': 0.54059829059829057 (before)</code><br />
<code>'SVC': 0.54059829059829057(after)</code>`</p>

<p>I continued model refinement using the smaller feature set (native_country removed) to make the models generalizable, but the decrease in precision/recall was considered. The ROC curve was plotted for the 4 short-listed estimators using the reduced feature list.</p>

<figure>
  <img src="/images/census_images/census_fig6.png" alt="Fig1" style='width:200%'>
  <figcaption>Figure 4. The ROC curve of selected estimators in predicting income class, 
  features reduced</figcaption>
</figure>

<p>Based on this analysis, the two most promising classifiers are the GradientBoostingClassifier and the LGBMClassifier. Both of these algorithms are &lsquo;boosted&rsquo; algorithms, which means they use multiple weakly-correlated features to classify the target. The GradientBoostingClassifier and the LGBMClassifier were selected for further tuning using GridSearchCV.</p>

<h3 id="perform-parameter-tuning-using-gridsearchcv-on-best-models">Perform parameter tuning using GridSearchCV on best models</h3>

<p>I tuned the hyper parameters of the GradientBoostingClassifier following the process outlined in this <a href="https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/">blog post</a>. The approach is to 1) hold learning rate constant, 2) optimize n_estimators (number of trees), 3) optimize other tree parameters, then 4) use the newly determined parameters to optimize learning rate and n_estimators in a combined GridSearch. This model is slow to train, so doing a succession of GridSearches for different sets of parameters is faster than doing one large search on a wide range of parameters. After I determined &lsquo;reasonable&rsquo; values for the hyper-parameters, I did a final combined GridSearch using a tight range of the &lsquo;optimized&rsquo; parameters to capture any unexpected interactions between parameters. See Section 5 of my <a href="https://github.com/katherinechandler/Census_Kaggle/tree/master">Census Income Classification</a> notebook for more detailed tuning information. The parameters of the tuned GradientBoostingClassifier are indicated below:</p>

<pre><code>GradientBoostingClassifier(learning_rate = 0.01,
                           n_estimators=700, 
                           max_depth=16, 
                           min_samples_split=450, 
                           min_samples_leaf= 2, 
                           max_features= 44, 
                           subsample = 1.0)
</code></pre>

<p>The performance of the tuned model was compared to a model using default parameter settings.</p>

<p><code>GradientBoostingClassifier performance:</code><br />
<code>The precision_score is 0.7985865724381626, default params</code><br />
<code>The recall_score is 0.5794871794871795, default params</code><br />
<code>The accuracy_score is 0.8642645101852799, default params</code></p>

<p><code>The precision_score is 0.7893639207507821, tuned parameters</code><br />
<code>The recall_score is 0.647008547008547, tuned parameters</code><br />
<code>The accuracy_score is 0.874091513972771, tuned parameters</code></p>

<p>In this final tuned model the accuracy score and recall score have increased significantly. The precision score, however, has gone down slightly. The tuned algorithm is identifying more of the relevant (greater than $50K households) targets, but at the cost of also identifying slightly more false cases (less than $50K) as positives. There is a tradeoff between precision and recall, but in this case the overall increase in model accuracy makes a precision decrease acceptable.</p>

<p>The LightGBMClassifier was tuned using a similar approach, though larger grids could be searched with GridSearchCV since this algorithm trains quickly. The parameters of the tuned LightGBMClassifier are indicated below:</p>

<pre><code>lgb.LGBMClassifier(learning_rate=0.05,
                   n_estimators=250,
                   reg_alpha=0.001,  
                   num_leaves=18,
                   max_depth=11,
                   min_data_in_leaf=7)
</code></pre>

<p>The performance of the tuned LGBMClassifier model was compared to a model using default parameter settings. As with the GradientBoostingClassifier, accuracy and recall had increased but precision score slightly decreased.</p>

<p><code>LGBMClassifier performance:</code><br />
<code>The precision_score is 0.78296988577362, default parameters</code><br />
<code>The recall_score is 0.6444444444444445, default parameters</code><br />
<code>The accuracy_score is 0.87204422151704, default parameters</code></p>

<p><code>The precision_score is 0.7904761904761904, tuned parameters</code><br />
<code>The recall_score is 0.6384615384615384, tuned parameters</code><br />
<code>The accuracy_score is 0.8728631384993346, tuned parameters</code></p>

<h3 id="identify-the-best-model">Identify the best model</h3>

<p>After testing a range of algorithms and parameters, the two best models were boosted classifiers: the GradientBoostingClassifier and the LGBMClassifier. After tuning algorithm hyper-parameters, the accuracy of the GradientBoostingClassifier was 87.4% and the accuracy of the LGBMClassifier was a close 87.3%. The GradientBoostingClassifier is the best algorithm for this task, correctly classifying 64.7% of households making more than $50K annually (recall_score = 0.6470). Of positive cases identified by this model, 78.9% are true positives (precision_score is 0.7894).</p>
<ul class="pa0">
  
</ul>
<div class="mt6">
        
      </div>
    </main>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-near-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://katherinechandler.io" >
    &copy; 2018 Home
  </a>
  







  <a href="https://github.com/katherinechandler" class="link-transition github link dib z-999 pt3 pt0-l mr2" title="Github link">
    <svg  height="32px"  style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
  <path d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z"/>
</svg>

  </a>


  </div>
</footer>

    

  <script src="/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
